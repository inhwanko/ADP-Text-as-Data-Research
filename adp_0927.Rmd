---
title: "ADP_wordfish"
author: "Inhwan Ko"
date: 'Sep 27, 2020'
output: html_document
---

```{r, include=FALSE}

# rm(list=ls())

setwd("C:/Users/inhwa/OneDrive - UW/Research/2020/ADP/2nd_analysis")

library(RTextTools)
library(tm)
library(tidyverse)
library(tidytext)
library(quanteda)

source("wordfish_1.3.r")

```


# 1. Data import and inspection

```{r}
library(readxl)
adp <- read_excel("adp_0927.xlsx")

length(unique(adp$group)) # 16 negotiation blocs
 
wordcount <- adp %>% 
  unnest_tokens(word, text) %>% # try different ones like bigrams and trigrams 
  anti_join(stop_words) %>% 
  count(word, sort=T)

wordcount
nrow(wordcount)

```

# 2. Wordfish 

```{r}
wordfish_dtm <- create_matrix(adp$text, language="english",
                     removeNumbers=T, stemWords=TRUE, 
                     removePunctuation=T, removeStopwords=T,
                     toLower=T, stripWhitespace=T)

tdm <- as.TermDocumentMatrix(wordfish_dtm) %>% 
  as.matrix()

wf <- wordfish(tdm, dir=c(118,115)) 

adp$score <- NA
adp$score <- wf$documents

write.csv(adp, "adp_with_scores.csv")

write.csv(wf$words, "wordweight.csv")
```

```{r}
bloc <- unique(adp$group)

score <- NA
for (i in 1:length(bloc)) {
  score[i] <- mean(adp$score[adp$group==bloc[i]])
}

score_result <- as.data.frame(cbind(bloc, score))
colnames(score_result) <- c("group","score")
score_result$score <- as.numeric(as.character(score_result$score))

score_result <- score_result %>% 
  arrange(desc(score))

write.csv(score_result, "score_result.csv")


```

Now, plotting the score results.  

```{r}
library(ggplot2)

ggplot(score_result, aes(reorder(group, -score), score)) +
  geom_point(aes(color=group), show.legend=F) +
  coord_flip() +
  geom_hline(yintercept=0, color="red") +
  ggtitle("Mean score of Negotiation Blocs") +
  xlab("Negotiation Blocs") +
  ylab("Mean Score")
  
```
Time-series of positional changes?

```{r}
groups_score <- list(NULL)
for (i in 1:length(bloc)) {
  groups_score[[i]] <- adp %>% 
    filter(group==bloc[i]) %>% 
    select(group,time,score) %>% 
    group_by(time) %>% 
    summarise(meanscore = mean(score))
  groups_score[[i]]$group <- NA
  groups_score[[i]]$group <- bloc[i]
}

groups_score_df <- data.frame(time=NA, meanscore=NA, group=NA)
for (i in 1:length(bloc)) {
  groups_score_df <- rbind(groups_score_df, groups_score[[i]])
}
groups_score_df <- na.omit(groups_score_df) 

library(directlabels)
ggplot(groups_score_df, aes(time, meanscore, group=group, color=group)) +
  geom_line() +
  geom_point(alpha=0.3) +
  scale_x_continuous(breaks=c(1:12)) +
  geom_dl(aes(label = group), method = list(dl.combine("last.points")), cex = 0.3) 



```


# 3. STM

## 1. Prep the STM

```{r}
library(stm)

colnames(adp)

stm_adp <- adp %>% 
  select(doc_id, text, session, part, time, date, year, group, score)

pre_stm_adp <- textProcessor(stm_adp$text, metadata=stm_adp)
dtm_stm_adp <- prepDocuments(pre_stm_adp$documents, pre_stm_adp$vocab,
                             pre_stm_adp$meta, lower.thresh = 0)
```

## 2. Find the optimized number for topics

```{r echo=T, results='hide'}
set.seed(2020)
k=c(3:20, 25, 30, 40, 50, 100)

dtm <- dtm_stm_adp

optimize_k <- searchK(dtm$documents, dtm$vocab, k, data=dtm$meta)
```

```{r}
plot(optimize_k)

```

## 3. Run the STM

```{r}
K <- 10
stm <- stm(dtm$documents, dtm$vocab, K,
           data=dtm$meta,
           seed=2020, init.type="Spectral")

labelTopics(stm, topics=1:K)

stm_corr <- topicCorr(stm)

stm_dt <- make.dt(stm, meta=dtm$meta) 
write.csv(stm_dt, "stm_dt.csv")

summary(lm(Topic2 ~ score.omega, stm_dt))
summary(lm(Topic9 ~ score.omega, stm_dt))

```

```{r}
stm_withscore <- stm(dtm$documents, dtm$vocab, K,
           data=dtm$meta, prevalence=~score,
           seed=2020, init.type="Spectral")

stm_withscore_corr <- topicCorr(stm_withscore)

result <- estimateEffect(c(2) ~ score, stm_withscore, stm_adp)
```





